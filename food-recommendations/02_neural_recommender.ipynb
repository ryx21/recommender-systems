{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Neural Recommender\n",
    "\n",
    "I want to be able to generate recommendations for a user based on all the other recipes they've liked. The basic requirement is that a new user should be able to select a sample of recipes they like and generate recommendations without re-training the model.\n",
    "* User embeddings\n",
    "* Item embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-Processing Recipe Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%colors nocolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>minutes</th>\n",
       "      <th>contributor_id</th>\n",
       "      <th>submitted</th>\n",
       "      <th>tags</th>\n",
       "      <th>nutrition</th>\n",
       "      <th>n_steps</th>\n",
       "      <th>steps</th>\n",
       "      <th>description</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>n_ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arriba   baked winter squash mexican style</td>\n",
       "      <td>137739</td>\n",
       "      <td>55</td>\n",
       "      <td>47892</td>\n",
       "      <td>2005-09-16</td>\n",
       "      <td>[60-minutes-or-less, time-to-make, course, mai...</td>\n",
       "      <td>[51.5, 0.0, 13.0, 0.0, 2.0, 0.0, 4.0]</td>\n",
       "      <td>11</td>\n",
       "      <td>[make a choice and proceed with recipe, depend...</td>\n",
       "      <td>autumn is my favorite time of year to cook! th...</td>\n",
       "      <td>[winter squash, mexican seasoning, mixed spice...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         name      id  minutes  \\\n",
       "0  arriba   baked winter squash mexican style  137739       55   \n",
       "\n",
       "   contributor_id   submitted  \\\n",
       "0           47892  2005-09-16   \n",
       "\n",
       "                                                tags  \\\n",
       "0  [60-minutes-or-less, time-to-make, course, mai...   \n",
       "\n",
       "                               nutrition  n_steps  \\\n",
       "0  [51.5, 0.0, 13.0, 0.0, 2.0, 0.0, 4.0]       11   \n",
       "\n",
       "                                               steps  \\\n",
       "0  [make a choice and proceed with recipe, depend...   \n",
       "\n",
       "                                         description  \\\n",
       "0  autumn is my favorite time of year to cook! th...   \n",
       "\n",
       "                                         ingredients  n_ingredients  \n",
       "0  [winter squash, mexican seasoning, mixed spice...              7  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import ast\n",
    "import math\n",
    "\n",
    "df_recipes = pd.read_csv('data/RAW_recipes.csv')\n",
    "for col in [\"tags\", \"nutrition\", \"steps\", \"ingredients\"]:\n",
    "    df_recipes[col] = df_recipes[col].apply(ast.literal_eval)\n",
    "\n",
    "df_recipes.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rory/miniconda3/envs/deep-learning/lib/python3.9/site-packages/sklearn/preprocessing/_function_transformer.py:345: UserWarning: With transform=\"pandas\", `func` should return a DataFrame to follow the set_output API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>minutes</th>\n",
       "      <th>contributor_id</th>\n",
       "      <th>submitted</th>\n",
       "      <th>tags</th>\n",
       "      <th>nutrition</th>\n",
       "      <th>n_steps</th>\n",
       "      <th>steps</th>\n",
       "      <th>description</th>\n",
       "      <th>...</th>\n",
       "      <th>F_sodium</th>\n",
       "      <th>F_protein</th>\n",
       "      <th>F_saturated_fat</th>\n",
       "      <th>F_carbohydrates</th>\n",
       "      <th>F_tags</th>\n",
       "      <th>F_steps</th>\n",
       "      <th>F_ingredients</th>\n",
       "      <th>F_id</th>\n",
       "      <th>F_name</th>\n",
       "      <th>F_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arriba   baked winter squash mexican style</td>\n",
       "      <td>137739</td>\n",
       "      <td>55</td>\n",
       "      <td>47892</td>\n",
       "      <td>2005-09-16</td>\n",
       "      <td>[60-minutes-or-less, time-to-make, course, mai...</td>\n",
       "      <td>[51.5, 0.0, 13.0, 0.0, 2.0, 0.0, 4.0]</td>\n",
       "      <td>11</td>\n",
       "      <td>[make a choice and proceed with recipe, depend...</td>\n",
       "      <td>autumn is my favorite time of year to cook! th...</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.972144</td>\n",
       "      <td>-1.403524</td>\n",
       "      <td>-2.027874</td>\n",
       "      <td>-0.567595</td>\n",
       "      <td>60-minutes-or-less time-to-make course main-in...</td>\n",
       "      <td>make a choice and proceed with recipe dependin...</td>\n",
       "      <td>winter squash mexican seasoning mixed spice ho...</td>\n",
       "      <td>137739</td>\n",
       "      <td>arriba   baked winter squash mexican style</td>\n",
       "      <td>autumn is my favorite time of year to cook! th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         name      id  minutes  \\\n",
       "0  arriba   baked winter squash mexican style  137739       55   \n",
       "\n",
       "   contributor_id   submitted  \\\n",
       "0           47892  2005-09-16   \n",
       "\n",
       "                                                tags  \\\n",
       "0  [60-minutes-or-less, time-to-make, course, mai...   \n",
       "\n",
       "                               nutrition  n_steps  \\\n",
       "0  [51.5, 0.0, 13.0, 0.0, 2.0, 0.0, 4.0]       11   \n",
       "\n",
       "                                               steps  \\\n",
       "0  [make a choice and proceed with recipe, depend...   \n",
       "\n",
       "                                         description  ...  F_sodium  \\\n",
       "0  autumn is my favorite time of year to cook! th...  ... -1.972144   \n",
       "\n",
       "   F_protein  F_saturated_fat  F_carbohydrates  \\\n",
       "0  -1.403524        -2.027874        -0.567595   \n",
       "\n",
       "                                              F_tags  \\\n",
       "0  60-minutes-or-less time-to-make course main-in...   \n",
       "\n",
       "                                             F_steps  \\\n",
       "0  make a choice and proceed with recipe dependin...   \n",
       "\n",
       "                                       F_ingredients    F_id  \\\n",
       "0  winter squash mexican seasoning mixed spice ho...  137739   \n",
       "\n",
       "                                       F_name  \\\n",
       "0  arriba   baked winter squash mexican style   \n",
       "\n",
       "                                       F_description  \n",
       "0  autumn is my favorite time of year to cook! th...  \n",
       "\n",
       "[1 rows x 35 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# handle nans\n",
    "df_recipes[\"description\"] = df_recipes[\"description\"].fillna(\"\")\n",
    "df_recipes = df_recipes.dropna(subset=[\"name\"])\n",
    "\n",
    "# Convert nutrition info to individual columns\n",
    "NUTRITION_COLS = ['calories', 'total_fat', 'sugar', 'sodium', 'protein', 'saturated_fat', 'carbohydrates']\n",
    "df_recipes[NUTRITION_COLS] = df_recipes[\"nutrition\"].tolist()\n",
    "\n",
    "# Preprocess numerical features\n",
    "NUMERICAL_FEATURES = [\"minutes\", \"n_steps\", \"n_ingredients\"] + NUTRITION_COLS\n",
    "TEXT_FEATURES = [\"ingredients\"]\n",
    "\n",
    "numerical_pipeline = make_pipeline(\n",
    "    FunctionTransformer(lambda x: np.sign(x) * np.log(np.abs(x)+1)),\n",
    "    StandardScaler()\n",
    ").set_output(transform=\"pandas\")\n",
    "\n",
    "df_recipes_numerical = numerical_pipeline.fit_transform(df_recipes[NUMERICAL_FEATURES])\n",
    "for col in NUMERICAL_FEATURES:\n",
    "    df_recipes[f\"F_{col}\"] = df_recipes_numerical[col]\n",
    "\n",
    "# Join lists of strings\n",
    "for col in [\"tags\", \"steps\", \"ingredients\"]:\n",
    "    df_recipes[f\"F_{col}\"] = df_recipes[col].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# Unchanged cols\n",
    "for col in [\"id\", \"name\", \"description\"]:\n",
    "    df_recipes[f\"F_{col}\"] = df_recipes[col]\n",
    "\n",
    "df_recipes.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 features:\n",
      "['F_minutes', 'F_n_steps', 'F_n_ingredients', 'F_calories', 'F_total_fat', 'F_sugar', 'F_sodium', 'F_protein', 'F_saturated_fat', 'F_carbohydrates', 'F_tags', 'F_steps', 'F_ingredients', 'F_id', 'F_name', 'F_description']\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [col for col in df_recipes.columns if col.startswith(\"F_\")]\n",
    "print(f\"{len(feature_cols)} features:\")\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-Processing Interaction Features\n",
    "\n",
    "In this section we pre-process user-recipe interactions into a format where we each interaction is eriched with the features for:\n",
    "1. The target recipe (i.e. one being interacted with)\n",
    "2. The set of recipes the user has interacted with *excluding* the target recipe (and ignoring time dependencies). I'll call these *context* recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>recipe_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38094</td>\n",
       "      <td>40893</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  recipe_id  rating\n",
       "0    38094      40893       4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_interactions = pd.read_csv('data/RAW_interactions.csv')\n",
    "df_interactions = df_interactions.drop([\"review\", \"date\"], axis=1)\n",
    "df_interactions.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Join recipe features onto the interactions by `recipe_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>recipe_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>id</th>\n",
       "      <th>F_minutes</th>\n",
       "      <th>F_n_steps</th>\n",
       "      <th>F_n_ingredients</th>\n",
       "      <th>F_calories</th>\n",
       "      <th>F_total_fat</th>\n",
       "      <th>F_sugar</th>\n",
       "      <th>F_sodium</th>\n",
       "      <th>F_protein</th>\n",
       "      <th>F_saturated_fat</th>\n",
       "      <th>F_carbohydrates</th>\n",
       "      <th>F_tags</th>\n",
       "      <th>F_steps</th>\n",
       "      <th>F_ingredients</th>\n",
       "      <th>F_id</th>\n",
       "      <th>F_name</th>\n",
       "      <th>F_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38094</td>\n",
       "      <td>40893</td>\n",
       "      <td>4</td>\n",
       "      <td>40893</td>\n",
       "      <td>2.322343</td>\n",
       "      <td>-1.177385</td>\n",
       "      <td>0.170344</td>\n",
       "      <td>-0.380026</td>\n",
       "      <td>-0.843167</td>\n",
       "      <td>-0.631142</td>\n",
       "      <td>0.533637</td>\n",
       "      <td>0.258756</td>\n",
       "      <td>-1.271503</td>\n",
       "      <td>0.188034</td>\n",
       "      <td>weeknight time-to-make course main-ingredient ...</td>\n",
       "      <td>combine beans , onion , chilies , 1 / 2 teaspo...</td>\n",
       "      <td>great northern beans yellow onion diced green ...</td>\n",
       "      <td>40893</td>\n",
       "      <td>white bean   green chile pepper soup</td>\n",
       "      <td>easy soup for the crockpot.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  recipe_id  rating     id  F_minutes  F_n_steps  F_n_ingredients  \\\n",
       "0    38094      40893       4  40893   2.322343  -1.177385         0.170344   \n",
       "\n",
       "   F_calories  F_total_fat   F_sugar  F_sodium  F_protein  F_saturated_fat  \\\n",
       "0   -0.380026    -0.843167 -0.631142  0.533637   0.258756        -1.271503   \n",
       "\n",
       "   F_carbohydrates                                             F_tags  \\\n",
       "0         0.188034  weeknight time-to-make course main-ingredient ...   \n",
       "\n",
       "                                             F_steps  \\\n",
       "0  combine beans , onion , chilies , 1 / 2 teaspo...   \n",
       "\n",
       "                                       F_ingredients   F_id  \\\n",
       "0  great northern beans yellow onion diced green ...  40893   \n",
       "\n",
       "                                 F_name                F_description  \n",
       "0  white bean   green chile pepper soup  easy soup for the crockpot.  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_interactions_joined = pd.merge(df_interactions, df_recipes[[\"id\"] + feature_cols], how='inner', left_on=['recipe_id'], right_on=['id'])\n",
    "df_interactions_joined.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Aggregate each column on `user_id` into lists, such that each each row in the aggregation table respresents the set of recipes a user interacts with. This is a table *context recipes* for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>recipe_id_list</th>\n",
       "      <th>rating_list</th>\n",
       "      <th>id_list</th>\n",
       "      <th>F_minutes_list</th>\n",
       "      <th>F_n_steps_list</th>\n",
       "      <th>F_n_ingredients_list</th>\n",
       "      <th>F_calories_list</th>\n",
       "      <th>F_total_fat_list</th>\n",
       "      <th>F_sugar_list</th>\n",
       "      <th>F_sodium_list</th>\n",
       "      <th>F_protein_list</th>\n",
       "      <th>F_saturated_fat_list</th>\n",
       "      <th>F_carbohydrates_list</th>\n",
       "      <th>F_tags_list</th>\n",
       "      <th>F_steps_list</th>\n",
       "      <th>F_ingredients_list</th>\n",
       "      <th>F_id_list</th>\n",
       "      <th>F_name_list</th>\n",
       "      <th>F_description_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1533</td>\n",
       "      <td>[116345, 32907, 14750, 24136, 63598, 83375, 35...</td>\n",
       "      <td>[5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "      <td>[116345, 32907, 14750, 24136, 63598, 83375, 35...</td>\n",
       "      <td>[0.4711129434497512, -0.5799463465672527, 1.28...</td>\n",
       "      <td>[0.4629144894108395, -0.0760934775964507, -1.1...</td>\n",
       "      <td>[1.552239356685329, -0.4107808961373495, -0.41...</td>\n",
       "      <td>[0.24415188448893518, 0.1864649534815458, -0.7...</td>\n",
       "      <td>[0.1955761015159676, 0.40097184283934584, -0.7...</td>\n",
       "      <td>[0.5205592569484367, 0.444796953040082, -0.360...</td>\n",
       "      <td>[0.3425659458571573, -0.2215192245871849, -0.1...</td>\n",
       "      <td>[1.054694179027774, 0.9451212086373081, -0.542...</td>\n",
       "      <td>[0.03462296668504492, 0.755679422126608, -0.44...</td>\n",
       "      <td>[0.0966917832022376, -0.392864824739948, -0.11...</td>\n",
       "      <td>[time-to-make course main-ingredient cuisine p...</td>\n",
       "      <td>[combine all cashew crust ingredients in a sma...</td>\n",
       "      <td>[tilapia fillets egg white flour lemon cashews...</td>\n",
       "      <td>[116345, 32907, 14750, 24136, 63598, 83375, 35...</td>\n",
       "      <td>[cashew crusted stuffed tilapia, indecent brea...</td>\n",
       "      <td>[this recipe was created for ready set cook 20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                     recipe_id_list  \\\n",
       "0     1533  [116345, 32907, 14750, 24136, 63598, 83375, 35...   \n",
       "\n",
       "                                         rating_list  \\\n",
       "0  [5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...   \n",
       "\n",
       "                                             id_list  \\\n",
       "0  [116345, 32907, 14750, 24136, 63598, 83375, 35...   \n",
       "\n",
       "                                      F_minutes_list  \\\n",
       "0  [0.4711129434497512, -0.5799463465672527, 1.28...   \n",
       "\n",
       "                                      F_n_steps_list  \\\n",
       "0  [0.4629144894108395, -0.0760934775964507, -1.1...   \n",
       "\n",
       "                                F_n_ingredients_list  \\\n",
       "0  [1.552239356685329, -0.4107808961373495, -0.41...   \n",
       "\n",
       "                                     F_calories_list  \\\n",
       "0  [0.24415188448893518, 0.1864649534815458, -0.7...   \n",
       "\n",
       "                                    F_total_fat_list  \\\n",
       "0  [0.1955761015159676, 0.40097184283934584, -0.7...   \n",
       "\n",
       "                                        F_sugar_list  \\\n",
       "0  [0.5205592569484367, 0.444796953040082, -0.360...   \n",
       "\n",
       "                                       F_sodium_list  \\\n",
       "0  [0.3425659458571573, -0.2215192245871849, -0.1...   \n",
       "\n",
       "                                      F_protein_list  \\\n",
       "0  [1.054694179027774, 0.9451212086373081, -0.542...   \n",
       "\n",
       "                                F_saturated_fat_list  \\\n",
       "0  [0.03462296668504492, 0.755679422126608, -0.44...   \n",
       "\n",
       "                                F_carbohydrates_list  \\\n",
       "0  [0.0966917832022376, -0.392864824739948, -0.11...   \n",
       "\n",
       "                                         F_tags_list  \\\n",
       "0  [time-to-make course main-ingredient cuisine p...   \n",
       "\n",
       "                                        F_steps_list  \\\n",
       "0  [combine all cashew crust ingredients in a sma...   \n",
       "\n",
       "                                  F_ingredients_list  \\\n",
       "0  [tilapia fillets egg white flour lemon cashews...   \n",
       "\n",
       "                                           F_id_list  \\\n",
       "0  [116345, 32907, 14750, 24136, 63598, 83375, 35...   \n",
       "\n",
       "                                         F_name_list  \\\n",
       "0  [cashew crusted stuffed tilapia, indecent brea...   \n",
       "\n",
       "                                  F_description_list  \n",
       "0  [this recipe was created for ready set cook 20...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_interactions_groupby = df_interactions_joined.groupby(\"user_id\").agg(list)\n",
    "# only keep groups with at least 2 \n",
    "df_interactions_groupby = df_interactions_groupby.reset_index()\n",
    "df_interactions_groupby = df_interactions_groupby.rename(columns={col: col + \"_list\" for col in df_interactions_groupby.columns if col != \"user_id\"})\n",
    "df_interactions_groupby = df_interactions_groupby.loc[df_interactions_groupby['recipe_id_list'].apply(len) > 2]\n",
    "df_interactions_groupby.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Left join the context recipe table back to the interaction table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>recipe_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>id</th>\n",
       "      <th>F_minutes</th>\n",
       "      <th>F_n_steps</th>\n",
       "      <th>F_n_ingredients</th>\n",
       "      <th>F_calories</th>\n",
       "      <th>F_total_fat</th>\n",
       "      <th>F_sugar</th>\n",
       "      <th>...</th>\n",
       "      <th>F_sodium_list</th>\n",
       "      <th>F_protein_list</th>\n",
       "      <th>F_saturated_fat_list</th>\n",
       "      <th>F_carbohydrates_list</th>\n",
       "      <th>F_tags_list</th>\n",
       "      <th>F_steps_list</th>\n",
       "      <th>F_ingredients_list</th>\n",
       "      <th>F_id_list</th>\n",
       "      <th>F_name_list</th>\n",
       "      <th>F_description_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38094</td>\n",
       "      <td>40893</td>\n",
       "      <td>4</td>\n",
       "      <td>40893</td>\n",
       "      <td>2.322343</td>\n",
       "      <td>-1.177385</td>\n",
       "      <td>0.170344</td>\n",
       "      <td>-0.380026</td>\n",
       "      <td>-0.843167</td>\n",
       "      <td>-0.631142</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.5336369886062798, 0.4751245248312128, -0.14...</td>\n",
       "      <td>[0.2587561767435663, 0.4764186786861827, -0.63...</td>\n",
       "      <td>[-1.271503074491975, -1.0734401952842334, -0.3...</td>\n",
       "      <td>[0.18803357608915794, 0.8077374177449108, -0.7...</td>\n",
       "      <td>[weeknight time-to-make course main-ingredient...</td>\n",
       "      <td>[combine beans , onion , chilies , 1 / 2 teasp...</td>\n",
       "      <td>[great northern beans yellow onion diced green...</td>\n",
       "      <td>[40893, 16954, 40753, 34513, 69545, 49064, 800...</td>\n",
       "      <td>[white bean   green chile pepper soup, black b...</td>\n",
       "      <td>[easy soup for the crockpot., one of my favori...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  recipe_id  rating     id  F_minutes  F_n_steps  F_n_ingredients  \\\n",
       "0    38094      40893       4  40893   2.322343  -1.177385         0.170344   \n",
       "\n",
       "   F_calories  F_total_fat   F_sugar  ...  \\\n",
       "0   -0.380026    -0.843167 -0.631142  ...   \n",
       "\n",
       "                                       F_sodium_list  \\\n",
       "0  [0.5336369886062798, 0.4751245248312128, -0.14...   \n",
       "\n",
       "                                      F_protein_list  \\\n",
       "0  [0.2587561767435663, 0.4764186786861827, -0.63...   \n",
       "\n",
       "                                F_saturated_fat_list  \\\n",
       "0  [-1.271503074491975, -1.0734401952842334, -0.3...   \n",
       "\n",
       "                                F_carbohydrates_list  \\\n",
       "0  [0.18803357608915794, 0.8077374177449108, -0.7...   \n",
       "\n",
       "                                         F_tags_list  \\\n",
       "0  [weeknight time-to-make course main-ingredient...   \n",
       "\n",
       "                                        F_steps_list  \\\n",
       "0  [combine beans , onion , chilies , 1 / 2 teasp...   \n",
       "\n",
       "                                  F_ingredients_list  \\\n",
       "0  [great northern beans yellow onion diced green...   \n",
       "\n",
       "                                           F_id_list  \\\n",
       "0  [40893, 16954, 40753, 34513, 69545, 49064, 800...   \n",
       "\n",
       "                                         F_name_list  \\\n",
       "0  [white bean   green chile pepper soup, black b...   \n",
       "\n",
       "                                  F_description_list  \n",
       "0  [easy soup for the crockpot., one of my favori...  \n",
       "\n",
       "[1 rows x 39 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_samples = pd.merge(df_interactions_joined, df_interactions_groupby, on=\"user_id\")\n",
    "df_samples.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: remove the target recipe from each set of context recipes for each interaction row.\n",
    "\n",
    "This caused me a lot of headaches, because it turns out the left join in step 4 resulted in only *references* to the aggregate rows produced step 3 being stored in the final joined table, `df_samples`. The data itself is only stored once. This means trying to directly modify the contents of a joined cell will modify it for all other values for that user. For example, removing an element from `F_minutes_list` for `user_1`, `recipe_a` has a side effect of removing the same elements from `F_minutes_list` in `user_1`, `recipe_b`. This is because both cells reference the same underlying object in memory (you can check this using the built in `id` function).\n",
    "\n",
    "This is why I resorted to writing a list in-place - but this is extremely slow taking ~0.1s per row (or ~25 hours for the whole thing), hence this ugly line of code:\n",
    "``` \n",
    "    df_samples.at[i, col] = [get_list_sample_val_or_pad(i, row[col], sample_idx, padding) for i in range(max_sequence)]\n",
    "```\n",
    "\n",
    "Other gotchas:\n",
    "* Using `df.apply()` here didn't work too well either...it didn't scale linearly with the number of rows I applied it to, which was a bit worrying. I haven't tried running it for longer than 2 hours though.\n",
    "* Another gotcha in `pandas` is trying to modify an element by *chained indexing* sometimes results in nothing happening in the original table. See [here](https://pandas.pydata.org/docs/user_guide/indexing.html#returning-a-view-versus-a-copy) for more details.\n",
    "* For the sake of brevity, I've truncated each context recipe set to just 20 (padding with default values) by sampling.\n",
    "\n",
    "Fow now, the remaining work is done on only 100 rows of data just to show the model is learning *something* and test that all the components for training, testing and inference work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "max_sequence = 20\n",
    "\n",
    "def get_list_sample_val_or_pad(i, lst, sample_idx, padding):\n",
    "    if i < len(sample_idx):\n",
    "        return lst[sample_idx[i]]\n",
    "    else:\n",
    "        return padding\n",
    "\n",
    "# for i in range(len(df_samples)): # this should take ~25 hours...need to figure out how to do this faster\n",
    "for i in range(100):\n",
    "    if (i+1) % 10000 == 0:\n",
    "        print(f\"Processed {i+1} rows\")\n",
    "    row = df_samples.iloc[i]\n",
    "    target_index = row[\"recipe_id_list\"].index(row[\"recipe_id\"])\n",
    "    \n",
    "    sequence_size = len(row[\"recipe_id_list\"])\n",
    "    sample_size = min(max_sequence, sequence_size)\n",
    "    sample_idx = random.sample(range(sample_size), sample_size)\n",
    "    # remove target index from sample\n",
    "    if target_index in sample_idx:\n",
    "        sample_idx.remove(target_index)\n",
    "\n",
    "    for col in row.index:\n",
    "        if col.endswith(\"_list\"):\n",
    "            # work out what value to pad rows with\n",
    "            if isinstance(row[col][0], str):\n",
    "                padding = \"\"\n",
    "            else:\n",
    "                padding = 0\n",
    "            df_samples.at[i, col] = [get_list_sample_val_or_pad(i, row[col], sample_idx, padding) for i in range(max_sequence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipe_id_list 20\n",
      "rating_list 20\n",
      "id_list 20\n",
      "F_minutes_list 20\n",
      "F_n_steps_list 20\n",
      "F_n_ingredients_list 20\n",
      "F_calories_list 20\n",
      "F_total_fat_list 20\n",
      "F_sugar_list 20\n",
      "F_sodium_list 20\n",
      "F_protein_list 20\n",
      "F_saturated_fat_list 20\n",
      "F_carbohydrates_list 20\n",
      "F_tags_list 20\n",
      "F_steps_list 20\n",
      "F_ingredients_list 20\n",
      "F_id_list 20\n",
      "F_name_list 20\n",
      "F_description_list 20\n"
     ]
    }
   ],
   "source": [
    "for key, val in df_samples.iloc[0].items():\n",
    "    if key.endswith(\"list\"):\n",
    "        print(key, len(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the `tf.data.Dataset` API to wrap the dataset, ready to be used for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "dataset = tf.data.Dataset.from_tensor_slices(df_samples.head(100).to_dict(\"list\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuring the Model\n",
    "\n",
    "Here I build a standard 2 tower model (similar to a the basic TF tutorial [here](https://www.tensorflow.org/recommenders/examples/basic_retrieval)), with a few twists:\n",
    "* Rather than learning a table of fixed embeddings for each user and movie, I dynamically generate an embedding for the user and recipe solely based on content and context based features. This should mean that if I add new users and new recipes, I won't need to retrain the model from scratch\n",
    "* A key piece of information that feeds into the user model is an aggregation over the set of other recipes they've liked. I share many of the embedding layers across the user and recipe towers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Text\n",
    "import tensorflow.python.keras.backend as K\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "\n",
    "NUMERICAL = [\n",
    "    'F_minutes',\n",
    "    'F_n_steps',\n",
    "    'F_n_ingredients',\n",
    "    'F_calories',\n",
    "    'F_sugar',\n",
    "    'F_total_fat',\n",
    "    'F_sodium',\n",
    "    'F_protein',\n",
    "    'F_saturated_fat',\n",
    "    'F_carbohydrates'\n",
    "]\n",
    "\n",
    "NUMERICAL_HISTORY = [f\"{num}_list\" for num in NUMERICAL]\n",
    "\n",
    "\n",
    "class PoolingTextEmbedder(tf.keras.Model):\n",
    "    \"\"\"Currently masking of padding tokens\"\"\" \n",
    "    def __init__(self, vocabulary_list=List[str], embedding_dim=16, max_tokens=10_000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.text_vectorizor = tf.keras.layers.TextVectorization(max_tokens=max_tokens)\n",
    "        self.text_vectorizor.adapt(vocabulary_list)\n",
    "        self.embedding_layer = tf.keras.layers.Embedding(max_tokens, embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.text_vectorizor(x)\n",
    "        x = self.embedding_layer(x)\n",
    "        x = tf.math.reduce_mean(x, axis=-2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UserModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, numerical_cols: List[str], text_embedding_layer: tf.keras.Model, output_dims: List[int]=[32]):\n",
    "        super().__init__()\n",
    "\n",
    "        # inputs\n",
    "        self.numerical_cols = numerical_cols\n",
    "        self.text_embedding_layer = text_embedding_layer\n",
    "        \n",
    "        # attention and pooling over sequence\n",
    "        self.attention_dim = len(numerical_cols) + text_embedding_layer.embedding_dim\n",
    "        self.key_layer = tf.keras.layers.Dense(self.attention_dim, activation=\"relu\", name=\"dense_key\")\n",
    "        self.query_layer = tf.keras.layers.Dense(self.attention_dim, activation=\"relu\", name=\"dense_query\")\n",
    "        self.attention_layer = tf.keras.layers.Attention(name=\"attention\")\n",
    "        \n",
    "        # Use the ReLU activation for all but the last layer\n",
    "        self.dense_layers = tf.keras.Sequential(name=\"user_dense_output\")\n",
    "        for dim in output_dims[:-1]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(dim, activation=\"relu\"))\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(output_dims[-1]))\n",
    "\n",
    "    def _pad_rank(self, x):\n",
    "        return tf.expand_dims(x, axis=-1)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        # Prepare text and numerical inputs\n",
    "        text_inputs = self.text_embedding_layer(self._pad_rank(inputs[\"F_ingredients_list\"]))\n",
    "        numerical_inputs = [self._pad_rank(inputs[f]) for f in self.numerical_cols]\n",
    "        x = tf.concat([text_inputs] + numerical_inputs, axis=-1)\n",
    "        # apply attention -> dense layers\n",
    "        x = self.attention_layer([self.query_layer(x), x, self.key_layer(x)])\n",
    "        x = tf.math.reduce_mean(x, axis=-2)\n",
    "        x = tf.reshape(x, [-1, self.attention_dim]) # hack required to handle single sample at inference\n",
    "        x = self.dense_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ItemModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, numerical_cols: List[str], text_embedding_layer: tf.keras.Model, output_dims: List[str]=[32]):\n",
    "        super().__init__()\n",
    "    \n",
    "        # inputs\n",
    "        self.numerical_cols = numerical_cols\n",
    "        self.text_embedding_layer = text_embedding_layer\n",
    "\n",
    "        # Use the ReLU activation for all but the last layer\n",
    "        self.dense_layers = tf.keras.Sequential(name=\"item_dense_output\")\n",
    "        for dim in output_dims[:-1]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(dim, activation=\"relu\"))\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(output_dims[-1]))\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        text_inputs = self.text_embedding_layer(inputs[\"F_ingredients\"])\n",
    "        numerical_inputs = [tf.expand_dims(inputs[f], axis=-1) for f in self.numerical_cols]\n",
    "        x = tf.concat([text_inputs] + numerical_inputs, axis=-1)\n",
    "        # apply dense layers\n",
    "        x = self.dense_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Based largely on this code: https://www.tensorflow.org/recommenders/examples/basic_retrieval\n",
    "class SimpleRetrievalModel(tfrs.Model):\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "        user_model: tf.keras.Model,\n",
    "        item_model: tf.keras.Model,\n",
    "\t\tcandidates\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\t\n",
    "\t\tself.user_model = user_model\n",
    "\t\tself.item_model = item_model\n",
    "\n",
    "\t\tself.task = tfrs.tasks.Retrieval(\n",
    "\t\t\tmetrics=tfrs.metrics.FactorizedTopK(\n",
    "\t\t\t\tcandidates=candidates.batch(128).map(self.item_model),\n",
    "\t\t\t\tks=(10,)\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\n",
    "\tdef compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "\t\tuser_embeddings = self.user_model(features)\n",
    "\t\titem_embeddings = self.item_model(features)\n",
    "\t\treturn self.task(user_embeddings, item_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User model output example shape: (2, 8)\n",
      "Item model output example shape: (2, 8)\n"
     ]
    }
   ],
   "source": [
    "ingredient_embedding_layer = PoolingTextEmbedder(dataset.map(lambda x: x[\"F_ingredients\"]), embedding_dim=16)\n",
    "\n",
    "user_model = UserModel(NUMERICAL_HISTORY, ingredient_embedding_layer, output_dims=[8])\n",
    "item_model = ItemModel(NUMERICAL, ingredient_embedding_layer, output_dims=[8])\n",
    "\n",
    "for batch in dataset.take(10).batch(2):\n",
    "    tester = batch\n",
    "    break\n",
    "\n",
    "print(\"User model output example shape:\", user_model(tester).shape)\n",
    "print(\"Item model output example shape:\", item_model(tester).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we train this for a few epochs on a tiny dataset, the training loss goes down which gives me confidence the model is learning something. I just need to now scale the model up and use the full training set. Since I'm developing this locally, this will require a bigger machine..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 2s 113ms/step - factorized_top_k/top_10_categorical_accuracy: 0.1875 - loss: 23.2488 - regularization_loss: 0.0000e+00 - total_loss: 23.2488\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 1s 133ms/step - factorized_top_k/top_10_categorical_accuracy: 0.2125 - loss: 21.8013 - regularization_loss: 0.0000e+00 - total_loss: 21.8013\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 1s 113ms/step - factorized_top_k/top_10_categorical_accuracy: 0.2125 - loss: 21.1540 - regularization_loss: 0.0000e+00 - total_loss: 21.1540\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 1s 112ms/step - factorized_top_k/top_10_categorical_accuracy: 0.1875 - loss: 20.8960 - regularization_loss: 0.0000e+00 - total_loss: 20.8960\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 1s 111ms/step - factorized_top_k/top_10_categorical_accuracy: 0.2250 - loss: 20.5401 - regularization_loss: 0.0000e+00 - total_loss: 20.5401\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 1s 110ms/step - factorized_top_k/top_10_categorical_accuracy: 0.2375 - loss: 20.1003 - regularization_loss: 0.0000e+00 - total_loss: 20.1003\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 1s 110ms/step - factorized_top_k/top_10_categorical_accuracy: 0.2500 - loss: 19.7063 - regularization_loss: 0.0000e+00 - total_loss: 19.7063\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 1s 110ms/step - factorized_top_k/top_10_categorical_accuracy: 0.2750 - loss: 19.2204 - regularization_loss: 0.0000e+00 - total_loss: 19.2204\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 1s 109ms/step - factorized_top_k/top_10_categorical_accuracy: 0.3000 - loss: 18.7336 - regularization_loss: 0.0000e+00 - total_loss: 18.7336\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 1s 110ms/step - factorized_top_k/top_10_categorical_accuracy: 0.2750 - loss: 18.2034 - regularization_loss: 0.0000e+00 - total_loss: 18.2034\n",
      "2/2 [==============================] - 1s 112ms/step - factorized_top_k/top_10_categorical_accuracy: 0.2500 - loss: 21.4536 - regularization_loss: 0.0000e+00 - total_loss: 21.4536\n"
     ]
    }
   ],
   "source": [
    "# create a tiny train and test set\n",
    "tf.random.set_seed(42)\n",
    "shuffled = dataset.shuffle(100, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80)\n",
    "test = shuffled.skip(80).take(20)\n",
    "\n",
    "cached_train = train.batch(10).cache()\n",
    "cached_test = test.batch(10).cache()\n",
    "\n",
    "# create a dataset of recipes to pass as candidate recommendations\n",
    "recipes_dataset = tf.data.Dataset.from_tensor_slices(df_recipes[[\"id\", \"name\"] + feature_cols].to_dict(\"list\"))\n",
    "\n",
    "model = SimpleRetrievalModel(\n",
    "    user_model=user_model,\n",
    "    item_model=item_model,\n",
    "    candidates=recipes_dataset.take(100)\n",
    ")\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "model.fit(cached_train, epochs=10)\n",
    "\n",
    "results = model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we use a brute force nearest neighbour search method to recommend the top N recipes. We could use `ScANN` for more efficient serving..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.layers.factorized_top_k.BruteForce at 0x7ffa80d7af40>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a model that takes in raw query features, and\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "# recommends movies out of the entire movies dataset.\n",
    "index.index_from_dataset(\n",
    "  tf.data.Dataset.zip((recipes_dataset.batch(20).map(lambda x: x[\"name\"]), recipes_dataset.batch(20).map(model.item_model)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       " array([[1.2600166, 1.2317915, 1.2290578, 1.2176449, 1.2156482, 1.2108135,\n",
       "         1.2101247, 1.2046136, 1.1976283, 1.189898 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 10), dtype=string, numpy=\n",
       " array([[b'really easy vanilla sugar', b'lavender white tea',\n",
       "         b'lemon verbena water', b'amarula coffee', b'apple tea',\n",
       "         b'americano', b'butterscotch tea',\n",
       "         b'vegan condensed milk substitute', b'white cactus',\n",
       "         b'la bou creamy dill dressing']], dtype=object)>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generating recommendations for a single test user\n",
    "index(test.take(1).get_single_element())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps: train a bigger model on the full dataset on a big machine...enter AWS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('deep-learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b3370f475946e8db7b3b3b72452614cbce4d51c227694df80ed27a1e2ab9497"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
